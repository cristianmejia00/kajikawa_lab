{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice_4_titech",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNinDEY2czu0tjHMlv16Z4p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cristianmejia00/kajikawa_lab/blob/master/Python-2021/Practice_4_titech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z2Uc5BWhnR0"
      },
      "source": [
        "# Methodology of Mathematical and Computational Analysis II  \n",
        "Graduate Major in Technology and Innovation Management  \n",
        "Tokyo Institute of Technology  \n",
        "2021 - Q2  \n",
        "Class 6, Programming exercise 2  \n",
        "\n",
        "Sasahara Kazutoshi, Mejia Cristian  \n",
        "contact: mejia.c.aa@m.titech.ac.jp   \n",
        "\n",
        "---\n",
        "Data and materials: https://drive.google.com/drive/folders/1qJzzQhdCYuzkqteBJxnmzglPMWaP9s_-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfe1ka420Bnc"
      },
      "source": [
        "![Picture1.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABIMAAABpCAMAAACeRRMyAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAGUExURe9HbwAAAMvjc5QAAAACdFJOU/8A5bcwSgAAAAlwSFlzAAAXEQAAFxEByibzPwAAAI1JREFUeF7twTEBAAAAwqD1T20MHyAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC4qAHaMwABy+5C8gAAAABJRU5ErkJggg==)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oASHZEvqh2j0"
      },
      "source": [
        "\n",
        "## Part 1: Fundamentals\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ap1d3RTuD5L"
      },
      "source": [
        "### Working with strings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMj-MCO1w249"
      },
      "source": [
        "# This is a 'string' (a.k.a 'text' or 'characters')\n",
        "'Hello world!'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ24omnlw7sl"
      },
      "source": [
        "# We can also use double quotation marks\n",
        "\"I'm Tokodai Taro\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srfn8iZoxAfZ"
      },
      "source": [
        "# Or combined when makes sense\n",
        "'She asked, \"How are you today?\"'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RcAzLwgxD6_"
      },
      "source": [
        "# It can be assigned as variable\n",
        "greeting = \"Hello, World!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBN46A96xMIW"
      },
      "source": [
        "# And printed in the console\n",
        "print(greeting)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeAsfN7pxe2M"
      },
      "source": [
        "# We can replace characters\n",
        "greeting.replace(\"!\", \"?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjitH4c8xNMp"
      },
      "source": [
        "# Strings can be concatenated with '+'\n",
        "statement = \"Hello, \" + \"World!\"\n",
        "print(statement)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A367NuTexRbH"
      },
      "source": [
        "# Don't forget to add spaces when using '+'\n",
        "print(\"This \" + \"is \" + \"a \" + \"longer \" + \"statement.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtnALjHDxVgi"
      },
      "source": [
        "# String can be part of a list\n",
        "words = ['this','is','a','list', 'of', 'words']\n",
        "words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVzVMwttyK-2"
      },
      "source": [
        "# We can join them with speace\n",
        "joined_text = \" \".join(words)\n",
        "joined_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHCzruhjzdAa"
      },
      "source": [
        "# Or split them back\n",
        "split_text = joined_text.split()\n",
        "split_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYTgidWy4OgH"
      },
      "source": [
        "ðŸ”´ #1\n",
        "#### Your turn!  \n",
        "Transform the followin paragraph into a list of words:\n",
        "\n",
        "Tokyo Institute of Technology is a national research university located in Greater Tokyo Area, Japan. Tokyo Tech is the largest institution for higher education in Japan dedicated to science and technology."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwMAZ6FU4n9a"
      },
      "source": [
        "# Your code here:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "244t-DNK1_I7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRWjGKTq2uD9"
      },
      "source": [
        "A key part of NLP is is text processing and transforming text into mathematical objects.\n",
        "NLTK provides various functions that help us transform the text into vectors. The most basic NLTK function for this purpose is tokenization, which splits a document into a list of units. These units could be words, alphabets, or sentences.  \n",
        "\n",
        "In the following sections we are going to use NLTK for several cleaning steps needed in text mining. \n",
        " \n",
        "  \n",
        "_______  \n",
        "*Note: If after loading the following blocks error appear, then click on \"restart runtime\" and click on 'play' again.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmUNbOqIhmIQ"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "! pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSJMqedR5_wG"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "Tokenization refer to the task of splitting the strings into it's components. We call each of those components a 'token'.  \n",
        "Is almost the same as 'term', or 'words'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsLTphSb6FZv"
      },
      "source": [
        "# The simplest tokenization is to just split the sentence by the space\n",
        "sentence = \"Who would have thought that computer programs would be analyzing human sentiments\"\n",
        "sentence.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HJzyPpz6Q-E"
      },
      "source": [
        "# But this can introduce problems, like the aphostrophe in English\n",
        "sentence = \"Let's travel to New York from Tokyo\"\n",
        "sentence.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5vBhKcZ6mG4"
      },
      "source": [
        "# Or other types of 'noise' introduced in social media communication\n",
        "sentence = \"Tokyo is a cool place!!! :P <3 #Awesome\"\n",
        "sentence.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4Bdo45V7UBq"
      },
      "source": [
        "# Let's do again a simple tokenization\n",
        "sentence = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in the USA.\"\n",
        "sentence.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j0oDNlz758J"
      },
      "source": [
        "# Standard tokenizer in NLP\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "sentence = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79ljsv338ZzR"
      },
      "source": [
        "# Tokenization for Twitter\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "sentence = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
        "tokenizer = TweetTokenizer(reduce_len=True, strip_handles=True)\n",
        "tokenizer.tokenize(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_omT4Yz_faH"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "Imagine bringing all of the words computer, computerization, and computerize into one word, compute. What happens here is called stemming. As part of stemming, a crude attempt is made to remove the inflectional forms of a word and bring them to a base form called the stem. The chopped-off pieces are referred to as affixes. In the preceding example, compute is the base form and the affixes are r, rization, and rize, respectively. One thing to keep in mind is that the stem need not be a valid word as we know it. For example, the\n",
        "word traditional would get stemmed to tradit, which is not a valid word in the English dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhuFF2Ut_KjQ"
      },
      "source": [
        "plurals = ['caresses', 'flies', 'dies', 'mules', 'died', 'agreed', 'owned',\n",
        "'humbled', 'sized', 'meeting', 'stating',\n",
        " 'siezing', 'itemization', 'traditional', 'reference', 'colonizer',\n",
        "'plotted', 'having', 'generously']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD7WmjBjAI6c"
      },
      "source": [
        "# Porter stemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "singles = [stemmer.stem(plural) for plural in plurals]\n",
        "print(' '.join(singles))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS6syVl1_s-l"
      },
      "source": [
        "# Snowball stemmer #Better.\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer2 = SnowballStemmer(language='english')\n",
        "singles = [stemmer2.stem(plural) for plural in plurals]\n",
        "print(' '.join(singles))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiU4B3RgA0lm"
      },
      "source": [
        "### Lemmatization  \n",
        "\n",
        "Unlike stemming, wherein a few characters are removed from words using crude methods,\n",
        "lemmatization is a process wherein the context is used to convert a word to its meaningful\n",
        "base form. It helps in grouping together words that have a common base form and so can\n",
        "be identified as a single item. The base form is referred to as the lemma of the word and is\n",
        "also sometimes known as the dictionary form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvFAPJhgA9BP"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "singles = [lemmatizer.lemmatize(plural) for plural in plurals]\n",
        "print(' '.join(singles))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y_HtSQ1B_eQ"
      },
      "source": [
        "# Other example\n",
        "tokens = ['We', 'are', 'putting', 'in', 'efforts', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(' '.join(lemmatized_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u06jM1oGJfmJ"
      },
      "source": [
        "However, the NLTK lemmatizer needs to know the Part of Speech (POS) of the words to work well. \n",
        "\n",
        "POS refers to the list of "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh6ePluJIXfI"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "##This is a common method which is widely used across the NLP community of practitioners and readers\n",
        "def get_part_of_speech_tags(token):\n",
        "    \"\"\"Maps POS tags to first character lemmatize() accepts.\n",
        "    We are focusing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNYmfWDVIlon"
      },
      "source": [
        "lemmatized_tokens = [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in tokens]\n",
        "print(' '.join(lemmatized_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwNom_GKHszc"
      },
      "source": [
        "# Use spacy lemmatizer\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "doc = nlp(\"We are putting in efforts to enhance our understanding of Lemmatization\")\n",
        "\" \".join([token.lemma_ for token in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c6N-IgAKGLK"
      },
      "source": [
        "### Stopwords\n",
        "\n",
        "Stopwords are words such as a, an, the, in, at, and so on that occur frequently in text corpora and do not carry a lot of information in most contexts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSUEvWGEKR3L"
      },
      "source": [
        "import nltk\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "print(stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcSPej7eOHg4"
      },
      "source": [
        "Since NLTK provides us with a list of stop words, we can simply look up this list and filter\n",
        "out stop words from our word list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkRTf0SWKwz9"
      },
      "source": [
        "newtokens=[word for word in tokens if word not in stopwords]\n",
        "print(newtokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRRf_8DNKyJd"
      },
      "source": [
        "### Case folding  \n",
        "Another strategy that helps with normalization is called case folding. As part of case folding, all the letters in the text corpus are converted to lowercase.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN_nBxjxK8OM"
      },
      "source": [
        "sentence = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
        "sentence = sentence.lower()\n",
        "sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Psa-M4UKd_Y"
      },
      "source": [
        "## Text preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuOXu59IT4o7"
      },
      "source": [
        "sentence = \"\"\"2000 2000 2020 2021 Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling. Harry Potter, and his friends fight against Lord Voldemort, a dark wizard who intends subjugate all wizards and Muggles (non-magical people).\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8aieMGsKlhV"
      },
      "source": [
        "# Get the tokens\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokens = tokenizer.tokenize(sentence.lower())\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3J4mgDaTcKU"
      },
      "source": [
        "# To lowercase\n",
        "tokens = [token.lower() for token in tokens]\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHo3PIsDTNGR"
      },
      "source": [
        "# Remove symbols\n",
        "from string import punctuation\n",
        "punctuation_list = set(punctuation)\n",
        "tokens = [token for token in tokens if token not in punctuation_list]\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFpLV3yHTRqM"
      },
      "source": [
        "# Remove stopwords\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_list = set(stopwords.words('english'))\n",
        "tokens = [token for token in tokens if token not in stopwords_list]\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V22RQHtTlQb"
      },
      "source": [
        "# Remove custom words\n",
        "mystopwords = [\"j.\", \"k.\", \"fantasy\"]\n",
        "tokens = [token for token in tokens if token not in mystopwords]\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtXyozUOTiQg"
      },
      "source": [
        "# Remove numbers\n",
        "tokens = [token for token in tokens if token.isalpha()]\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9rxxa7uTsVn"
      },
      "source": [
        "# Stemming (or lemmatization)\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "tokens = [stemmer.stem(token) for token in tokens]\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHzZqgKwtSrf"
      },
      "source": [
        "For ilustration purposes we have done, everything step-by-step in this notebook.  In practice, is recommended to have a function that aggregates all these steps at once. So we can reuse it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDss5VSajACk"
      },
      "source": [
        "# Putting everything together\n",
        "\n",
        "# Load libraries\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# Initiate values\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "punctuation_list = set(punctuation)\n",
        "stopwords_list = set(stopwords.words('english'))\n",
        "my_stopwords = [\"j.\", \"k.\", \"fantasy\"]\n",
        "\n",
        "# Define a function\n",
        "def preprocessing(document):\n",
        "  tokens = tokenizer.tokenize(document.lower())\n",
        "  tokens = [token.lower() for token in tokens]\n",
        "  tokens = [token for token in tokens if token not in punctuation_list]\n",
        "  tokens = [token for token in tokens if token not in stopwords_list]\n",
        "  tokens = [token for token in tokens if token not in my_stopwords]\n",
        "  tokens = [token for token in tokens if token.isalpha()]\n",
        "  tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "  clean_document = \" \".join(tokens)\n",
        "  return clean_document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkqW8886kOXw"
      },
      "source": [
        "# Let's recap\n",
        "# This is the original sentence\n",
        "sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0jKTx0umFKo"
      },
      "source": [
        "# This is the clean sentence\n",
        "preprocessing(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FbSRvCR3_v9"
      },
      "source": [
        "ðŸ”´ #2\n",
        "#### Your turn!  \n",
        "Pick any text in English and clean it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-e9cxop4-qC"
      },
      "source": [
        "# Your code here:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4OfOOGBX0PV"
      },
      "source": [
        "# Other cases (Optional)\n",
        "import re\n",
        "\n",
        "# Remove numbers within a text in the token\n",
        "# For instance: \"4files\" -> \"files\"\n",
        "pattern = '[0-9]'\n",
        "tokens = [re.sub(pattern, '', token) for token in tokens]\n",
        "\n",
        "# Remove symbols within a text in the token\n",
        "# For instance: \"mejia@titec.com\" -> \"mejiatitechcom\"\n",
        "from string import punctuation\n",
        "tokens = [re.sub(punctuation, '', token) for token in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxxc3oujoUxH"
      },
      "source": [
        "## Analyzing one document\n",
        "\n",
        "In this section, we compute simple stats from the example sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va-NPW7HQt_I"
      },
      "source": [
        "# Get the term frequencies (TF)\n",
        "from collections import Counter\n",
        "bag_of_words = Counter(tokens)\n",
        "bag_of_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FACnqlSQQvkH"
      },
      "source": [
        "# Most common terms \n",
        "bag_of_words.most_common() # Sorts everything\n",
        "bag_of_words.most_common(4) # Sorts and shows only the top 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MATYxHbZoDg"
      },
      "source": [
        "# Number of time the term \"harri\" appears\n",
        "bag_of_words['harri']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDXjJZp3Zx6y"
      },
      "source": [
        "# Length of the vocabulary (i.e. number of unique terms)\n",
        "len(bag_of_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xqfejU20NDk"
      },
      "source": [
        "![Picture2.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABIMAAABpCAMAAACeRRMyAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAGUExURf/RZgAAAOt77DUAAAACdFJOU/8A5bcwSgAAAAlwSFlzAAAXEQAAFxEByibzPwAAAI1JREFUeF7twTEBAAAAwqD1T20MHyAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC4qAHaMwABy+5C8gAAAABJRU5ErkJggg==)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ox-xAwqp83o"
      },
      "source": [
        "## Part 2: Analyzing multiple documents\n",
        "\n",
        "Now, we can move to analyze larger datasets with multiple documents.  \n",
        "In this exercise we received a dataset compsed of 300 academic articles. The dataset contains the titles, abstract, author and publication years.  \n",
        "\n",
        "Here, we are going to use the title and abstract to try to infer how many topics are hidden in this dataset.\n",
        "\n",
        "You can download the datasets here:  \n",
        "https://drive.google.com/drive/folders/1qJzzQhdCYuzkqteBJxnmzglPMWaP9s_-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxWluvUShcbA"
      },
      "source": [
        "# Load libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggTEWIm1hdvN"
      },
      "source": [
        "# Load the dataset\n",
        "dataset = pd.read_csv(\"papers.csv\")\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBqB1_cGvLC7"
      },
      "source": [
        "# If you have problem uploading the data manually, uncomment the following two lines and run this cell.\n",
        "#dataset = pd.read_csv(\"https://raw.githubusercontent.com/cristianmejia00/kajikawa_lab/master/Python-2021/papers.csv\")\n",
        "#dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y6VqlG7qikz"
      },
      "source": [
        "We see there are some 'Nan' values in the columns. This causes problems when doing text mining. Hence, we need to replace them with an empty string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlGbPD3U1vP9"
      },
      "source": [
        "# Replace NaN to empty characters\n",
        "dataset = dataset.replace(np.nan, '', regex=True)\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le_wI6060Sae"
      },
      "source": [
        "# Analyze Title and Abstract\n",
        "papers_text = dataset['TI'] + \" \" + dataset[\"AB\"]\n",
        "corpus = pd.Series(papers_text)\n",
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2tNN81X1cCC"
      },
      "source": [
        "# Check a single document by location\n",
        "corpus.iloc[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c2olLyxq8VS"
      },
      "source": [
        "We need to clean the text. In the previous section we defined a cleaning function with the options we want to clean. Here, we just apply that function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4aS7HJU2bBx"
      },
      "source": [
        "# Clean each document\n",
        "clean_corpus = []\n",
        "for document in corpus:\n",
        "  clean_corpus.append(preprocessing(document))\n",
        "clean_corpus = pd.Series(clean_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bszk6C2J3QZo"
      },
      "source": [
        "# Check a single clean document by location\n",
        "clean_corpus.iloc[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bE5Uy1BgFeb"
      },
      "source": [
        "# Term Frequency (TF)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "tf_vectorizer = CountVectorizer()\n",
        "tf_matrix = tf_vectorizer.fit_transform(clean_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EoT26vL38Ck"
      },
      "source": [
        "print(tf_vectorizer.get_feature_names())\n",
        "print(tf_matrix.toarray())\n",
        "print(tf_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpQRCtS7OMiJ"
      },
      "source": [
        "# Term Frequency (TF) consider only top 500 most frequent terms\n",
        "tf_vectorizer = CountVectorizer(max_features = 500)\n",
        "tf_matrix = tf_vectorizer.fit_transform(clean_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUunMCS_OXm7"
      },
      "source": [
        "print(tf_vectorizer.get_feature_names())\n",
        "print(tf_matrix.toarray())\n",
        "print(tf_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0yPwFMBgN9e"
      },
      "source": [
        "# Term Frequency Inverse Document Frequency (TFIDF)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features = 500)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(clean_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF75SsRA4RTK"
      },
      "source": [
        "print(tfidf_vectorizer.get_feature_names())\n",
        "print(tfidf_matrix.toarray())\n",
        "print(tfidf_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4EW8yK0rVYg"
      },
      "source": [
        "### Document similarity  \n",
        "\n",
        "In this section, we use text mining to extract the documents that are the most similar to another. Here, we are using the cosine similarity which the most well-spread measure in text mining.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWEijo7PX_Ak"
      },
      "source": [
        "# Cosine similarity of documents:\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0f64K6GYPQs"
      },
      "source": [
        "cosine_matrix.shape\n",
        "print(cosine_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olPsZ1M-a1Yx"
      },
      "source": [
        "# Convert to dataframe\n",
        "cosine_df = pd.DataFrame(cosine_matrix, \n",
        "                  columns = dataset[\"TI\"], \n",
        "                  index=dataset[\"TI\"])\n",
        "cosine_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7srXnn_gyBt"
      },
      "source": [
        "# Find the most similar documents when we know the title or the location of the document\n",
        "# Find the most similar articles to \"High-performance medicine: the convergence of human and artificial intelligence\"\n",
        "\n",
        "doc_index = 1 #cosine_df.columns.get_loc(\"High-performance medicine: the convergence of human and artificial intelligence\")\n",
        "simmilar_docs = cosine_df.iloc[:, doc_index]\n",
        "simmilar_docs.sort_values(ascending=False).head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCRN9hsu8Ld6"
      },
      "source": [
        "ðŸ”´ #3\n",
        "#### Your turn!  \n",
        "Find the 10 most similar articles to:\n",
        "\"Emerging applications of bedside 3D printing in plastic surgery\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_D_P_c48KRR"
      },
      "source": [
        "# Your code here:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw_tMt5-rteb"
      },
      "source": [
        "### Topic Classification using Topic Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-6cFl7gxdn1"
      },
      "source": [
        "# Topic Models\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "\n",
        "no_topics = 3\n",
        "dictionary = tf_vectorizer.get_feature_names()\n",
        "\n",
        "# Run NMF\n",
        "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf_matrix)\n",
        "\n",
        "# Run LDA\n",
        "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnrN1kAHxxhH"
      },
      "source": [
        "def display_topics(model, token_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print (\"Topic %d:\" % (topic_idx))\n",
        "        print (\" \".join([token_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "no_top_words = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRAkq9E_xxnr"
      },
      "source": [
        "display_topics(nmf, dictionary, no_top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCf532C8xxtl"
      },
      "source": [
        "display_topics(lda, dictionary, no_top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFkvpzcr5FEH"
      },
      "source": [
        "! pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEr4k_NZ89Yd"
      },
      "source": [
        "import pyLDAvis.sklearn\n",
        "\n",
        "tf_vectorizer = CountVectorizer(max_features = 500)\n",
        "tf_matrix = tf_vectorizer.fit_transform(clean_corpus)\n",
        "panel = pyLDAvis.sklearn.prepare(lda, tf_matrix, tf_vectorizer, mds='tsne')\n",
        "pyLDAvis.display(panel)\n",
        "# Note: in the visualization, topics changed the order. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MofYZ1wT_LZO"
      },
      "source": [
        "### Document classification  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxjLKw6_xxze"
      },
      "source": [
        "# \"Climate change\" from Wikipedia\n",
        "new_document = \"Climate change includes both global warming driven by human-induced emissions of greenhouse gases and the resulting large-scale shifts in weather patterns. Though there have been previous periods of climatic change, since the mid-20th century humans have had an unprecedented impact on Earth's climate system and caused change on a global scale\"\n",
        "\n",
        "clean_new_document = preprocessing(new_document)\n",
        "clean_new_document = pd.Series(clean_new_document)\n",
        "tf_clean_new_document = tf_vectorizer.transform(clean_new_document)\n",
        "lda_prediction = lda.transform(tf_clean_new_document)\n",
        "print(lda_prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiI5clcM0UoB"
      },
      "source": [
        "![Picture3.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABIMAAABqCAMAAAAY0WGcAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAGUExURQbWoAAAADjgyS4AAAACdFJOU/8A5bcwSgAAAAlwSFlzAAAXEQAAFxEByibzPwAAAI1JREFUeF7twQEBAAAAgiD/r25IQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABwpwbetwAB5xmsMAAAAABJRU5ErkJggg==)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBG4mERmF8vQ"
      },
      "source": [
        "## Part 3: Sentiment Analysis  \n",
        "\n",
        "**Valence Aware Dictionary and sEntiment Reasoner (VADER)** is a recently developed lexicon-based sentiment analysis tool whose accuracy is shown to be much greater than the existing lexicon-based sentiment analyzers. This model was developed by computer science professors from Georgia Tech and they have published the methodology of building the lexicon in their very easy-to-read [paper](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf). It improves on other sentiment analyzers by including colloquial language terms, emoticons, slang, acronyms, and so on, which are used generously in social media. It also factors in the intensity of words rather than classifying them as simply positive or negative.\n",
        "\n",
        "Here, we can see that VADER outputs the negative score, neutral score, and positive score and then aggregates them to calculate the compound score. The compound score is what we are interested in. Any score greater than 0.05 is considered positive, while less than -0.05 is considered negative\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-TnxVngn26V"
      },
      "source": [
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "vader_analyzer = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBOXL_6ARn0z"
      },
      "source": [
        "# Test some sentences\n",
        "text1 = \"In three years, everyone will be happy.\"\n",
        "text2 = \"I am so excited about the concert.\"\n",
        "text3 = \"I do not like this car.\"\n",
        "vader_analyzer.polarity_scores(text3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSM8e3d5oNS3"
      },
      "source": [
        "### Sentiment Analysis of Trump Tweets  \n",
        "\n",
        "We are goin to analyze a dataset of tweets tweeted by former U.S. president Trump. The time coverage is from 2015 to 2020.  \n",
        "\n",
        "We will observer the trend of sentiment of his tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Khu9qCt3qhd"
      },
      "source": [
        "# Load the dataset. \n",
        "tweets = pd.read_csv(\"trump_tweets.csv\", parse_dates=True)\n",
        "tweets.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5j_tKGGv7Y0"
      },
      "source": [
        "# If you have problem uploading the data manually, uncomment the following two lines and run this cell.\n",
        "# dataset = pd.read_csv(\"https://raw.githubusercontent.com/cristianmejia00/kajikawa_lab/master/Python-2021/trump_tweets.csv\")\n",
        "# dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qDxCqpyHUgD"
      },
      "source": [
        "# How many tweets are there?\n",
        "len(tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCb-a4f3MHr4"
      },
      "source": [
        "# Add the sentiment scores for Trump tweets to the dataset\n",
        "tweets['polarity_scores'] = tweets.text.apply(vader_analyzer.polarity_scores)\n",
        "tweets['vader_compound'] = tweets.polarity_scores.apply(lambda x: x['compound'])\n",
        "tweets['vader_neg'] = tweets.polarity_scores.apply(lambda x: x['neg'])\n",
        "tweets['vader_neu'] = tweets.polarity_scores.apply(lambda x: x['neu'])\n",
        "tweets['vader_pos'] = tweets.polarity_scores.apply(lambda x: x['pos'])\n",
        "tweets.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qakigm9IMHyZ"
      },
      "source": [
        "# Grouping sentiment scores by day to get average daily sentiment scores\n",
        "from datetime import datetime as dt\n",
        "dt_sentiment_dmy = tweets.vader_compound.groupby(tweets.date_dmy).mean().reset_index()\n",
        "dt_sentiment_dmy['date_dmy'] = dt_sentiment_dmy.date_dmy.apply(lambda x: dt.strptime(x, '%Y-%m-%d'))\n",
        "dt_sentiment_dmy.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvUOdTLiNjyx"
      },
      "source": [
        "# Plot the compound score by day\n",
        "import seaborn as sns\n",
        "sns.lineplot(x = \"date_dmy\", y = \"vader_compound\", data = dt_sentiment_dmy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5PXscxBNjtf"
      },
      "source": [
        "# Group each sentiment score by days\n",
        "dt_all_sentiment = tweets[['vader_pos', 'vader_neu', 'vader_neg', 'vader_compound']].groupby(tweets.date_dmy).mean().reset_index()\n",
        "dt_all_sentiment['date_dmy'] = dt_all_sentiment.date_dmy.apply(lambda x: dt.strptime(x, '%Y-%m-%d'))\n",
        "dt_all_sentiment.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B069tiTNjnk"
      },
      "source": [
        "# Obtain statistics of each sentiment\n",
        "dt_all_sentiment.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B45BOvDVQYPd"
      },
      "source": [
        "# Plot the scores of negative tweets\n",
        "sns.lineplot(x = \"date_dmy\", y = \"vader_neg\", data = dt_all_sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO1pr6A70aZU"
      },
      "source": [
        "![Picture5.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABIMAAABqCAYAAAAvD5GuAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsQAAA7EAZUrDhsAAALtSURBVHhe7dgxAcAgEMDALyarq+oqBxZc5G6Jhzzf++8BAAAAIGHdAgAAABBgBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAABBiBgEAAACEmEEAAAAAIWYQAAAAQIgZBAAAAJAxcwDxQwOScq1jlAAAAABJRU5ErkJggg==)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_Vgg2Jgl97r"
      },
      "source": [
        "## Part 4: Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB9qbsW8h6Ap"
      },
      "source": [
        "# Import the library and load the model\n",
        "# This can take upto 5 min to load\n",
        "import gensim.downloader as api\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8a3gX0yUJbk"
      },
      "source": [
        "# Check the vocabulary size\n",
        "len(word_vectors.wv.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcvIurKdTW7N"
      },
      "source": [
        "# Check the vector size\n",
        "word_vectors.vector_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IglU8jC2TWzz"
      },
      "source": [
        "# Extract similar keywords\n",
        "word_vectors.most_similar('tokyo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFJGVrbiUUh4"
      },
      "source": [
        "# Test a typical example of Word2Vec\n",
        "result = word_vectors.most_similar(positive=['man', 'queen'], negative=['king'], topn=1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSu76RWvUZpM"
      },
      "source": [
        "# Let's try other escenario\n",
        "result = word_vectors.most_similar(positive=['sushi', 'spain'], negative=['japan'], topn=5)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9UmnG6Fj5Kw"
      },
      "source": [
        "# change 'sports' to 'chicago'\n",
        "word_vectors.doesnt_match(\"tokyo sports washington miami\".split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfyLRZ3Plx_h"
      },
      "source": [
        "## References  \n",
        "\n",
        "Books:\n",
        "* Hands-On Python Natural Language Processing by Aman Kedia Mayank Rasu\n",
        "* Natural Language Processing in Action by Hobson Lane, Cole Howard, and Hannes Max Hapke\n",
        "\n",
        "Others:\n",
        "* https://voyant-tools.org/\n",
        "* https://blog.mlreview.com/topic-modeling-with-scikit-learn-e80d33668730\n",
        "* https://shravan-kuchkula.github.io/topic-modeling/get-the-top-15-keywords-from-each-topic\n",
        "* https://github.com/naingthet/trump-tweets-nlp\n",
        "* https://radimrehurek.com/gensim/models/word2vec.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7TBYfL4jMUy"
      },
      "source": [
        "# Note.\n",
        "# Cosine simmilarity of terms:\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_matrix = cosine_similarity(np.transpose(tfidf_matrix), numpy.transpose(tfidf_matrix))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}